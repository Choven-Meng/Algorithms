

## 机器学习算法选择

&emsp;&emsp;对于分类问题，如何选择哪一个机器学习算法？如果真的在乎精度accuracy，最好的方法是通过交叉验证对各个算法进行测试比较，然后调整参数确保每个算法达到最优解，最后选择最好的一个。  

**1. 数据集大小**

偏差：描述的是预测值的期望E与真实值T之间的差距。偏差越大，越偏离真实数据。  
方差：描述的是预测值P的变化范围，离散程度，也就是离其期望值E的距离。方差越大，数据的分布越分散。   
模型的真实误差是两者之和。error = bias + variance  

如果是小训练集，高偏差/低方差的分类器(朴素贝叶斯)要优于低偏差/高方差分类器(KNN)，因为后者会过拟合。但是，当数据集增长，低偏差/高方差分类器分类器就会逐渐表现出优势(它们有较低的渐近误差).

### 一些常见算法的优势

[**1. 朴素贝叶斯**]()

属于生成式模型，如果注有条件独立性假设，朴素贝叶斯分类器的收敛速度将快于判别模型，如逻辑回归，所以你只需要较少的训练数据即可。即使NB条件独立假设不成立，NB分类器在实践中仍然表现的很出色。它的主要缺点是它不能学习特征件的相互作用（例如，它不能学习出虽然你喜欢Brad Pitt和Tom Cruise的电影，但是你不喜欢他们在一起演的电影）。   
优点：对小规模的数据表现很好，适合多分类任务，适合增量式训练。   
缺点：对输入数据的表达形式很敏感。

[**2. Logistic Regression逻辑回归]()   

属于判别式模型，有很多正则化模型的方法（L0，L1，L2, etc），而且你不必像在用朴素贝叶斯那样担心你的特征是否相关。与决策树与SVM机相比，你还会得到一个不错的概率解释，你甚至可以轻松地利用新数据来更新模型（使用在线梯度下降算法）。如果你需要一个概率架构（比如，简单地调节分类阈值，指明不确定性，或者是要获得置信区间），或者你希望以后将更多的训练数据快速整合到模型中去，那么使用它吧。   
**Sigmoid函数：**&emsp;&emsp;f(x) = 1 / (1+e<sup>-x</sup>)   
优点：实现简单，广泛的应用于工业问题上；分类是计算量非常小，速度很快，存储资源低；便利的观测样本概率分数；对逻辑回归而言，多重共线性不是问题，可以结合L2正则化来解决该问题。   
缺点：当特征空间很大时，逻辑回归的性能不好；容易欠拟合，一般准确率不高；不能很好的处理大量多类特征或变量；只能处理两分类问题(在此基础上衍生出的softmax)可以用于多分类，且必须线性可分；对于非线性特征，需要进行转换。
